{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d189d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ROOT_PATH = '/home/nandhini/NandhiniMathivananRnD/inputs/facial-keypoints-detection'\n",
    "OUTPUT_PATH = '/home/nandhini/NandhiniMathivananRnD/outputs'\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "LR = 0.0001\n",
    "EPOCHS = 3\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#train test split\n",
    "TEST_SPLIT = 0.2\n",
    "SHOW_DATASET_PLOT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f67316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import config\n",
    "import utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "resize = 96\n",
    "\n",
    "def train_test_split(csv_path, split):\n",
    "    df_data = pd.read_csv(csv_path)\n",
    "    # drop all the rows with missing values\n",
    "    df_data = df_data.dropna()\n",
    "    len_data = len(df_data)\n",
    "    # calculate the validation data sample length\n",
    "    valid_split = int(len_data * split)\n",
    "    # calculate the training data samples length\n",
    "    train_split = int(len_data - valid_split)\n",
    "    training_samples = df_data.iloc[:train_split][:]\n",
    "    valid_samples = df_data.iloc[-valid_split:][:]\n",
    "    print(f\"Training sample instances: {len(training_samples)}\")\n",
    "    print(f\"Validation sample instances: {len(valid_samples)}\")\n",
    "    return training_samples, valid_samples\n",
    "\n",
    "\n",
    "class FaceKeypointDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.data = samples\n",
    "        # get the image pixel column only\n",
    "        self.pixel_col = self.data.Image\n",
    "        # store the pixel values after extracting them from the Image column\n",
    "        self.image_pixels = []\n",
    "        for i in tqdm(range(len(self.data))):\n",
    "            # pixel values by space as they are space separated in the CSV file as well\n",
    "            img = self.pixel_col.iloc[i].split(' ')\n",
    "            self.image_pixels.append(img)\n",
    "        self.images = np.array(self.image_pixels, dtype='float32')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # reshape the images into their original 96x96 dimensions\n",
    "        image = self.images[index].reshape(96, 96)\n",
    "        # extract the original height and width of the images\n",
    "        orig_w, orig_h = image.shape\n",
    "        # resize the image into `resize` defined above\n",
    "        image = cv2.resize(image, (resize, resize))\n",
    "        # again reshape to add grayscale channel format\n",
    "        image = image.reshape(resize, resize, 1)\n",
    "        image = image / 255.0\n",
    "        # transpose for getting the channel size to index 0\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        # get the keypoints\n",
    "        keypoints = self.data.iloc[index][:30]\n",
    "        keypoints = np.array(keypoints, dtype='float32')\n",
    "        # reshape the keypoints\n",
    "        keypoints = keypoints.reshape(-1, 2)\n",
    "        # rescale keypoints according to image resize\n",
    "        keypoints = keypoints * [resize / orig_w, resize / orig_h]\n",
    "        return {\n",
    "            'image': torch.tensor(image, dtype=torch.float),\n",
    "            'keypoints': torch.tensor(keypoints, dtype=torch.float),\n",
    "        }\n",
    "\n",
    "    # get the training and validation data samples\n",
    "training_samples, valid_samples = train_test_split(f\"{config.ROOT_PATH}/training/training.csv\",\n",
    "                                                       config.TEST_SPLIT)\n",
    "    # initialize the dataset - `FaceKeypointDataset()`\n",
    "print('\\n-------------- PREPARING DATA --------------\\n')\n",
    "train_data = FaceKeypointDataset(training_samples)\n",
    "valid_data = FaceKeypointDataset(valid_samples)\n",
    "print('\\n-------------- DATA PREPRATION DONE --------------\\n')\n",
    "    # prepare data loaders\n",
    "train_loader = DataLoader(train_data,\n",
    "                              batch_size=config.BATCH_SIZE,\n",
    "                              shuffle=True)\n",
    "valid_loader = DataLoader(valid_data,\n",
    "                              batch_size=config.BATCH_SIZE,\n",
    "                              shuffle=False)\n",
    "\n",
    "if config.SHOW_DATASET_PLOT:\n",
    "     utils.dataset_keypoints_plot(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8df6800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import config\n",
    "\n",
    "\n",
    "def valid_keypoints_plot(image, outputs, orig_keypoints, epoch):\n",
    "    # detach the image, keypoints, and output tensors from GPU to CPU\n",
    "    image = image.detach().cpu()\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    orig_keypoints = orig_keypoints.detach().cpu().numpy()\n",
    "    # just get a single datapoint from each batch\n",
    "    img = image[0]\n",
    "    output_keypoint = outputs[0]\n",
    "    orig_keypoint = orig_keypoints[0]\n",
    "    img = np.array(img, dtype='float32')\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    # reshape it into the original 96Ã—96 dimensions\n",
    "    img = img.reshape(96, 96)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "\n",
    "    output_keypoint = output_keypoint.reshape(-1, 2)\n",
    "    orig_keypoint = orig_keypoint.reshape(-1, 2)\n",
    "    for p in range(output_keypoint.shape[0]):\n",
    "        plt.plot(output_keypoint[p, 0], output_keypoint[p, 1], 'r.')\n",
    "        plt.text(output_keypoint[p, 0], output_keypoint[p, 1], f\"{p}\")\n",
    "        plt.plot(orig_keypoint[p, 0], orig_keypoint[p, 1], 'g.')\n",
    "        plt.text(orig_keypoint[p, 0], orig_keypoint[p, 1], f\"{p}\")\n",
    "    plt.savefig(f\"{config.OUTPUT_PATH}/val_epoch_{epoch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def test_keypoints_plot(images_list, outputs_list):\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(len(images_list)):\n",
    "        outputs = outputs_list[i]\n",
    "        image = images_list[i]\n",
    "        outputs = outputs.cpu().detach().numpy()\n",
    "        outputs = outputs.reshape(-1, 2)\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        for p in range(outputs.shape[0]):\n",
    "                plt.plot(outputs[p, 0], outputs[p, 1], 'r.')\n",
    "                plt.text(outputs[p, 0], outputs[p, 1], f\"{p}\")\n",
    "        plt.axis('off')\n",
    "    plt.savefig(f\"{config.OUTPUT_PATH}/test_output.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def dataset_keypoints_plot(data):\n",
    "    plt.figure(figsize=(20, 40))\n",
    "    for i in range(30):\n",
    "        sample = data[i]\n",
    "        img = sample['image']\n",
    "        img = np.array(img, dtype='float32')\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        img = img.reshape(96, 96)\n",
    "        plt.subplot(5, 6, i + 1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        keypoints = sample['keypoints']\n",
    "        for j in range(len(keypoints)):\n",
    "            plt.plot(keypoints[j, 0], keypoints[j, 1], 'r.')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58e2a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceKeypointModel(nn.Module):\n",
    "    def __init__(self, freeze_resnet = False):\n",
    "        super(FaceKeypointModel, self).__init__()\n",
    "        \n",
    "        # Convert 1 filter 3 filter because resnet accepts 3 filter only\n",
    "        self.conv1 = nn.Conv2d( in_channels=1, out_channels=3, kernel_size=(3, 3), stride=1, padding=1, padding_mode='zeros' )\n",
    "        \n",
    "        # Resnet Architecture\n",
    "        self.resnet18 = models.resnet18(pretrained=True)\n",
    "        if freeze_resnet:\n",
    "            for param in self.resnet18.parameters():\n",
    "                param.requires_grad = False\n",
    "        # replacing last layer of resnet\n",
    "        # by default requires_grad in a layer is True\n",
    "        self.resnet18.fc = nn.Linear(self.resnet18.fc.in_features, 384) \n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(384, 30) \n",
    "        self.variance = nn.Linear(384,1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y0 = self.conv1(x)\n",
    "        y1 = self.resnet18(y0)\n",
    "        y_relu = self.relu(y1)\n",
    "        out= self.linear1(y_relu)\n",
    "        var = F.softplus(self.variance(y_relu))\n",
    "        return out,var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bed3a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627d50f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4601dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3889c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b4685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1157553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccdc27c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dafdb53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
